{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8198a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook or Python script\n",
    "# ------------------------------------\n",
    "# Title: Explore TPCH Sample Database\n",
    "# Description:\n",
    "#   This script inspects the sample \"tpch\" database available in Databricks.\n",
    "#   It lists available databases, enumerates all tables in \"tpch\",\n",
    "#   prints their schemas, and displays sample records.\n",
    "# ------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca65865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required packages\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create or get existing Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: List all available databases\n",
    "# ---------------------------------------------------\n",
    "print(\"=== AVAILABLE DATABASES ===\")\n",
    "databases = spark.catalog.listDatabases()\n",
    "for db in databases:\n",
    "    print(f\"- {db.name}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: List all tables in the 'tpch' database\n",
    "# ---------------------------------------------------\n",
    "print(\"=== TABLES IN 'tpch' DATABASE ===\")\n",
    "tables = spark.catalog.listTables(\"tpch\")\n",
    "\n",
    "# Display as a Pandas DataFrame for readability\n",
    "tables_df = pd.DataFrame(\n",
    "    [(t.name, t.tableType) for t in tables],\n",
    "    columns=[\"Table\", \"Type\"]\n",
    ")\n",
    "print(tables_df)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a2745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 3: Inspect the schema of a single table\n",
    "# ---------------------------------------------------\n",
    "# Example: Inspect the 'customer' table\n",
    "print(\"=== SCHEMA: tpch.customer ===\")\n",
    "customer_df = spark.table(\"tpch.customer\")\n",
    "customer_df.printSchema()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 4: Display a small data sample\n",
    "# ---------------------------------------------------\n",
    "# NOTE: In Databricks notebooks, use `display(customer_df.limit(5))`\n",
    "# In a plain Python script, use `.show()`\n",
    "print(\"=== SAMPLE DATA: tpch.customer ===\")\n",
    "customer_df.show(5)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e92f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 5: Inspect schemas of all tables (optional)\n",
    "# ---------------------------------------------------\n",
    "print(\"=== ALL TABLE SCHEMAS IN 'tpch' ===\")\n",
    "for t in tables:\n",
    "    print(f\"--- {t.name} ---\")\n",
    "    df = spark.table(f\"tpch.{t.name}\")\n",
    "    df.printSchema()\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
