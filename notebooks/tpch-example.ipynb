{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook or Python script\n",
    "# ------------------------------------\n",
    "# Title: Explore TPCH Sample Database (Auto Catalog Detection)\n",
    "# Description:\n",
    "#   This script explores the TPCH sample dataset in Databricks.\n",
    "#   It detects the correct catalog (e.g. \"samples.tpch\"), lists tables,\n",
    "#   prints their schemas, and previews example records.\n",
    "# ------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 0: Initialize Spark session\n",
    "# ---------------------------------------------------\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"=== CHECKING AVAILABLE CATALOGS ===\")\n",
    "catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "print(\"Catalogs found:\", catalogs)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff96141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: Detect which catalog contains the TPCH schema\n",
    "# ---------------------------------------------------\n",
    "target_schema = None\n",
    "target_catalog = None\n",
    "\n",
    "for catalog in catalogs:\n",
    "    schemas = [row.databaseName for row in spark.sql(f\"SHOW SCHEMAS IN {catalog}\").collect()]\n",
    "    if \"tpch\" in schemas:\n",
    "        target_catalog = catalog\n",
    "        target_schema = \"tpch\"\n",
    "        break\n",
    "\n",
    "if not target_catalog:\n",
    "    raise ValueError(\"Could not find schema 'tpch' in any available catalog.\")\n",
    "\n",
    "print(f\"Using catalog: {target_catalog}, schema: {target_schema}\")\n",
    "print()\n",
    "\n",
    "# Set active catalog and schema\n",
    "spark.sql(f\"USE CATALOG {target_catalog}\")\n",
    "spark.sql(f\"USE {target_schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: List tables in the TPCH schema\n",
    "# ---------------------------------------------------\n",
    "print(f\"=== TABLES IN {target_catalog}.{target_schema} ===\")\n",
    "tables = spark.catalog.listTables(f\"{target_catalog}.{target_schema}\")\n",
    "\n",
    "tables_df = pd.DataFrame(\n",
    "    [(t.name, t.tableType) for t in tables],\n",
    "    columns=[\"Table\", \"Type\"]\n",
    ")\n",
    "print(tables_df)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 3: Inspect the schema of a sample table\n",
    "# ---------------------------------------------------\n",
    "sample_table = \"customer\"\n",
    "\n",
    "print(f\"=== SCHEMA: {target_catalog}.{target_schema}.{sample_table} ===\")\n",
    "df = spark.table(f\"{target_catalog}.{target_schema}.{sample_table}\")\n",
    "df.printSchema()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59aebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 4: Display a small data sample\n",
    "# ---------------------------------------------------\n",
    "print(f\"=== SAMPLE DATA: {target_catalog}.{target_schema}.{sample_table} ===\")\n",
    "df.show(5)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b470805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 5: Inspect all table schemas (optional)\n",
    "# ---------------------------------------------------\n",
    "print(f\"=== ALL TABLE SCHEMAS IN {target_catalog}.{target_schema} ===\")\n",
    "for t in tables:\n",
    "    print(f\"--- {t.name} ---\")\n",
    "    tdf = spark.table(f\"{target_catalog}.{target_schema}.{t.name}\")\n",
    "    tdf.printSchema()\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
