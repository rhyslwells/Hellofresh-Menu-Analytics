## 2. Simple Daily Refresh Pipeline

#data_engineering #ml_process #orchestration

**Objective**
Understand repeatable jobs and idempotent data writes.

**Project**
Build a notebook that refreshes a dataset once per day.

**Steps**

* Read a source file or API snapshot.
* Add a load timestamp.
* Overwrite or merge into a Delta table.
* Schedule as a Databricks Job.

**Key Concepts**
Idempotency, overwrite vs merge, jobs, parameters.

**Deliverable**

* Scheduled job
* Delta table with ingestion metadata


---

## 7. External Data Integration

#data_engineering #integration

**Objective**
Learn how Databricks interacts with external systems.

**Project**
Read from an external source (Google Sheets, S3, REST API) and persist to Delta.

**Steps**

* Connect using a connector or HTTP call.
* Normalize schema.
* Persist to a governed table.

**Key Concepts**
External locations, credentials, ingestion patterns.

**Deliverable**

* External ingestion notebook
* Delta table as system of record

Yes. They overlap conceptually, but they differ along a few important dimensions that are worth making explicit.

---

## Core distinction

### Project 2: **Scheduled Refresh Pipeline**

#data_engineering #orchestration

**Focus**
Operational mechanics of *repeatable execution* inside Databricks.

**Primary questions explored**

* How do I make a notebook safe to run repeatedly?
* How do I schedule and parameterise execution?
* How do I control overwrite vs merge semantics?

**Typical source**

* Static file
* Snapshot export
* Already-landed data

**Learning centre**
$Idempotency$, jobs, scheduling, write modes.

---

### Project 7: **External Data Integration**

#data_engineering #integration

**Focus**
Boundary between Databricks and *external systems*.

**Primary questions explored**

* How do I authenticate to external services?
* How do I handle rate limits, schemas, and API failures?
* Where do credentials and secrets live?

**Typical source**

* REST API
* Google Sheets
* Object storage in another account

**Learning centre**
Connectors, secrets, external locations, network boundaries.

---

## Why they *feel* the same early on

At beginner scale, both often collapse into:

> “Read data → write Delta → run daily”

That similarity is real. The distinction only becomes clear once you hit:

* Authentication failures
* Schema drift
* Partial loads
* Rate limits or pagination

---

## How to collapse them (recommended for a first pass)

For a first Databricks project, you can deliberately **merge them**:

### Combined Intro Project

#data_engineering #integration #orchestration

**Project**
Ingest a Google Sheet or simple API **once per day** into a Delta table.

**You learn simultaneously**

* External access (credentials, connectors)
* Repeatable execution (jobs, idempotent writes)

This is often the most realistic “day one” Databricks experience.

---

## When to split them again

Split them once you want to reason about:

* **Pipelines** as reusable operational assets
* **Connectors** as fragile, failure-prone boundaries

At that point:

* Project 2 becomes about *execution semantics*
* Project 7 becomes about *integration risk management*

---

## Mental model

* Project 2 answers: *“How do I run this safely every day?”*
* Project 7 answers: *“How do I get data from somewhere that doesn’t belong to me?”*

---

If you want, I can rewrite the original list to:

* Remove redundancy entirely, or
* Make the distinctions sharper using a single evolving project rather than multiple toy ones

Which approach do you prefer?
