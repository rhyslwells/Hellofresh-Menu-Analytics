
## 1. Core principle

Treat the API as a **slowly evolving upstream system** and your database as the **system of record**.

You will:

1. Pull data on a fixed cadence (weekly).
2. Store immutable raw snapshots.
3. Derive stable, analytics-ready tables.
4. Build downstream projects that *depend on historical change*, not just current state.

This gives you time, versioning, and reproducibility.

---

## 2. Data ingestion design

### 2.1 Pull strategy (weekly)

**Cadence:** once per week
**Scope per pull:**

* Recipes
* Menus
* Reference data (ingredients, allergens, tags, labels)

Even if reference data “rarely changes”, pull it anyway to detect drift.

### 2.2 API extraction pattern

For each endpoint:

* Paginate exhaustively
* Respect rate limits
* Persist raw responses verbatim

Key metadata to attach per pull:

* $pull_date$
* $endpoint$
* $page$
* $locale$

---

## 3. Database layering

Use a **three-layer model**. This maps cleanly to Obsidian notes and blog narratives.

---

### 3.1 Bronze layer — raw snapshots

**Purpose:** reproducibility and audit

**Schema (example):**

```
bronze_api_responses
- pull_date
- endpoint
- locale
- payload_json
```

Properties:

* Append-only
* Never updated
* One row per API page

This allows:

* Backfills
* Reprocessing with new logic
* Debugging upstream changes

---

### 3.2 Silver layer — normalized entities

**Purpose:** stable relational representation

Example tables:

* `recipes`
* `ingredients`
* `recipe_ingredient_bridge`
* `allergens`
* `recipe_allergen_bridge`
* `menus`
* `menu_recipe_bridge`

Key design decision:

* Primary keys include **source ID**
* Add:

  * $first_seen_date$
  * $last_seen_date$
  * $is_active$

This enables **slowly changing dimensions** without heavy machinery.

---

### 3.3 Gold layer — analytical facts

**Purpose:** projects and consumption

Examples:

* Weekly menu composition
* Recipe reuse metrics
* Ingredient overlap scores
* Allergen density per menu

These tables are:

* Recomputed each pull
* Derived only from silver

---

## 4. Weekly update mechanics

Each weekly run follows the same deterministic steps:

1. Fetch all endpoints
2. Append to bronze
3. Rebuild silver:

   * Detect new entities
   * Update $last_seen_date$
   * Mark missing entities inactive
4. Recompute gold

No in-place mutation of bronze. Minimal mutation of silver.

---

## 5. First downstream project (recommended)

Build a project that **only works because the data evolves weekly**.

### Project: Recipe and menu drift analysis

**Questions:**

* How stable are menus week-to-week?
* Which recipes persist longest?
* Which ingredients are increasingly common?

**Core metrics:**

* $recipe_survival_weeks$
* $menu_overlap_ratio$
* $ingredient_trend_slope$

This forces you to:

* Use time explicitly
* Validate historical correctness
* Trust your ingestion logic
---

## 7. Technology-agnostic but realistic stack

You can implement this with:

* Python for ingestion
* SQL for transformation
* Any analytical database

The important part is **design discipline**, not tooling.

---


## 2. Simple Daily Refresh Pipeline

#data_engineering #ml_process #orchestration

**Objective**
Understand repeatable jobs and idempotent data writes.

**Project**
Build a notebook that refreshes a dataset once per day.

**Steps**

* Read a source file or API snapshot.
* Add a load timestamp.
* Overwrite or merge into a Delta table.
* Schedule as a Databricks Job.

**Key Concepts**
Idempotency, overwrite vs merge, jobs, parameters.

**Deliverable**

* Scheduled job
* Delta table with ingestion metadata


---

## 7. External Data Integration

#data_engineering #integration

**Objective**
Learn how Databricks interacts with external systems.

**Project**
Read from an external source (Google Sheets, S3, REST API) and persist to Delta.

**Steps**

* Connect using a connector or HTTP call.
* Normalize schema.
* Persist to a governed table.

**Key Concepts**
External locations, credentials, ingestion patterns.

**Deliverable**

* External ingestion notebook
* Delta table as system of record

Yes. They overlap conceptually, but they differ along a few important dimensions that are worth making explicit.

---

## Core distinction

### Project 2: **Scheduled Refresh Pipeline**

#data_engineering #orchestration

**Focus**
Operational mechanics of *repeatable execution* inside Databricks.

**Primary questions explored**

* How do I make a notebook safe to run repeatedly?
* How do I schedule and parameterise execution?
* How do I control overwrite vs merge semantics?

**Typical source**

* Static file
* Snapshot export
* Already-landed data

**Learning centre**
$Idempotency$, jobs, scheduling, write modes.

---

### Project 7: **External Data Integration**

#data_engineering #integration

**Focus**
Boundary between Databricks and *external systems*.

**Primary questions explored**

* How do I authenticate to external services?
* How do I handle rate limits, schemas, and API failures?
* Where do credentials and secrets live?

**Typical source**

* REST API
* Google Sheets
* Object storage in another account

**Learning centre**
Connectors, secrets, external locations, network boundaries.

---

## Why they *feel* the same early on

At beginner scale, both often collapse into:

> “Read data → write Delta → run daily”

That similarity is real. The distinction only becomes clear once you hit:

* Authentication failures
* Schema drift
* Partial loads
* Rate limits or pagination

---

## How to collapse them (recommended for a first pass)

For a first Databricks project, you can deliberately **merge them**:

### Combined Intro Project

#data_engineering #integration #orchestration

**Project**
Ingest a Google Sheet or simple API **once per day** into a Delta table.

**You learn simultaneously**

* External access (credentials, connectors)
* Repeatable execution (jobs, idempotent writes)

This is often the most realistic “day one” Databricks experience.

---

## When to split them again

Split them once you want to reason about:

* **Pipelines** as reusable operational assets
* **Connectors** as fragile, failure-prone boundaries

At that point:

* Project 2 becomes about *execution semantics*
* Project 7 becomes about *integration risk management*

---

## Mental model

* Project 2 answers: *“How do I run this safely every day?”*
* Project 7 answers: *“How do I get data from somewhere that doesn’t belong to me?”*

---

* Remove redundancy entirely, or
* Make the distinctions sharper using a single evolving project rather than multiple toy ones

---------------------------------


https://console.hfresh.info/docs/get-started

https://console.hfresh.info/docs

How to interact with api data?

Using "Fetch Web Page"

Fetched 2 resources

From the hfresh API documentation, here's what you can obtain:

**Main Data Resources:**

1. **Recipes**
   - List recipes (with search/filter capabilities)
   - Get individual recipe details

2. **Menus**
   - List weekly menus
   - Get individual menu details with recipes

3. **Reference Data**
   - **Tags** – Recipe categories and tags
   - **Labels** – Recipe labels (dietary, preparation style, etc.)
   - **Allergens** – Allergen information
   - **Ingredients** – Available ingredients list
   - **Countries** – Supported locales and countries

**Key API Details:**
- **Rate limit:** 60 requests/minute per user
- **Pagination:** 10-200 results per page (default 50)
- **Localization:** Supports multiple locales (e.g., en-GB, de-DE) for language/region-specific data
- **Authentication:** Bearer token required
- **Format:** JSON responses

**Available Downloads:**
- OpenAPI 3.0 (Swagger) specification
- Postman collection

This is a **HelloFresh recipe database API** that provides access to recipes, menus, and associated metadata like ingredients, allergens, and tags. You can build a comprehensive food/recipe data warehouse from this.

--------------------------

I want:

hfresh\silver_data.db to be saved in hfresh\output.

I want hfresh\gold_analytics.db to saved in hfresh\output.

As a final output i want hfresh\output\reports\weekly_report_2026-01-23.md
I want the most recent version also be the README.md maybe using a gitaction to do this.

Instead of saving it to hfresh\bronze_data\2026-01-23

I want this to be done in databricks populating a database instead of hfresh\bronze_data\2026-01-23.

I want this to be in databricks. 

I want to populate weekly hfresh\output\reports\weekly_report_2026-01-23.md.
as the final output, I want the report to include to include the charts from hfresh\output\charts
I dont want to save the weekly chart, just overwrite these. 

I want the reports to include results and charts that take into account the the recent weeks data, and historical data, so that it shows the the use that it gets the most recnt data with the api.

-----------------------------------

I want this document to be a blueprint for the project.